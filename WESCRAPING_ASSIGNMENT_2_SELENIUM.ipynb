{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc212b1e",
   "metadata": {},
   "source": [
    "# Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272089e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2aa053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6911946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first connect to the driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\User\\Downloads\\chromedriver_win32 (1)\\chromedriver.exe\")\n",
    "\n",
    "# Git Website\n",
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd4334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input \")\n",
    "designation.send_keys('Data Analyst')\n",
    "\n",
    "location=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys('Bangalore')\n",
    "\n",
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef28f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title = []\n",
    "job_location = []\n",
    "company_name = []\n",
    "experience_required = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89a585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping Job Title from the given Page\n",
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "    \n",
    "# scraping Job Location from the given page\n",
    "location_tags=driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location) \n",
    "    \n",
    "    \n",
    "# scraping Job company from the given page\n",
    "company_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)    \n",
    "    \n",
    "    \n",
    "# scraping Job experience from the given page\n",
    "experience_tags=driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span\")\n",
    "for i in experience_tags[0:10]:\n",
    "    experience=i.text\n",
    "    experience_required.append(experience) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bda503",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(job_title),len(job_location),len(company_name),len(experience_required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3545abb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating data frame\n",
    "df=pd.DataFrame({'job_title':job_title,'job_location':job_location,'company_name':company_name,'experience_required':experience_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e446622",
   "metadata": {},
   "source": [
    "# Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff57043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les first connect tot he driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\User\\Downloads\\chromedriver_win32 (1)\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf649b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input \")\n",
    "designation.send_keys('Data Scientist')\n",
    "\n",
    "\n",
    "location=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys('Bangalore')\n",
    "\n",
    "\n",
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc444d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title = []\n",
    "job_location = []\n",
    "company_name = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb484210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping Job Title from the given Page\n",
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "    \n",
    "# scraping Location from the given Page\n",
    "location_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "# scraping Location from the given Page\n",
    "company_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b36f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(job_title),len(job_location),len(company_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8393980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating data frame\n",
    "df=pd.DataFrame({'job_title':job_title,'job_location':job_location,'company_name':company_name})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ace06f0",
   "metadata": {},
   "source": [
    "# Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f617f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les first connect tot he driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\User\\Downloads\\chromedriver_win32 (1)\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input \")\n",
    "designation.send_keys('Data Scientist')\n",
    "\n",
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c608c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ticking on Delhi/NCR \n",
    "driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div/section[1]/div[2]/div[5]/div[2]/div[3]/label/p/span[1]\").click()\n",
    "\n",
    "# Ticking on 3-6 lakhs Salary\n",
    "driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div/section[1]/div[2]/div[6]/div[2]/div[2]/label/p/span[1]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6a082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title = []\n",
    "job_location = []\n",
    "company_name= []\n",
    "experience_required = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c97cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping Job Title from the given Page\n",
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "    \n",
    "# scraping Job Location from the given page\n",
    "location_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location) \n",
    "    \n",
    "# scraping Job company from the given page\n",
    "company_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company) \n",
    "    \n",
    "# scraping Job experience from the given page\n",
    "experience_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "for i in experience_tags[0:10]:\n",
    "    experience=i.text\n",
    "    experience_required.append(experience) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63076d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(job_title),len(job_location),len(company_name),len(experience_required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c84ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating data frame\n",
    "df=pd.DataFrame({'job_title':job_title,'job_location':job_location,'company_name':company_name,'experience_required':experience_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11320c0d",
   "metadata": {},
   "source": [
    "# Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and\n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the\n",
    "required data as usual.\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then\n",
    "click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100 sunglasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0ca8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les first connect tot he driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\User\\Downloads\\chromedriver_win32 (1)\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0ab4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "designation.send_keys('sunglasses')\n",
    "\n",
    "search=driver.find_element(By.CLASS_NAME,\"L0Z3Pu\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c20b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brands = []\n",
    "Product_Description = []\n",
    "Price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a0df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6cb452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping URl of 100 Sunglasses\n",
    "URL=[]\n",
    "for i in range(0,3):\n",
    "    Sunglasses_url=driver.find_elements(By.XPATH,'//a[@class=\"_2UzuFa\"]')\n",
    "    for i in Sunglasses_url:\n",
    "        URL.append(i.get_attribute('href'))\n",
    "    # Clicking on NEXT button at end of page\n",
    "    driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]').click()\n",
    "    time.sleep(4)\n",
    "    \n",
    "URL[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06cd260",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand = []\n",
    "Product_Description = []\n",
    "Price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e98485",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in URL[:100]:\n",
    "    driver.get(i)\n",
    "    try:\n",
    "        brand=driver.find_element(By.XPATH,\"//span[@class='G6XhRU']\").text\n",
    "        Brand.append(brand)\n",
    "        description=driver.find_element(By.XPATH,\"//span[@class='B_NuCI']\").text\n",
    "        Product_Description.append(description)\n",
    "        price=driver.find_element(By.XPATH,\"//div[@class='_30jeq3 _16Jk6d']\").text\n",
    "        Price.append(price)\n",
    "      \n",
    "    except NoSuchElementException:\n",
    "        Brand.append(\"NaN\")\n",
    "        Product_Description.append(\"NaN\")\n",
    "        Price.append(\"NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e5b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating data frame\n",
    "df=pd.DataFrame({'Brand':Brand,'Product_Description':Product_Description,'Price':Price})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740b1f4d",
   "metadata": {},
   "source": [
    "# Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.flipkart.com/\n",
    "2. Enter “iphone 11” in “Search” field .\n",
    "3. Then click the search button.\n",
    "As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a37e0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les first connect tot he driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\User\\Downloads\\chromedriver_win32 (1)\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4efc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "products=driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "products.send_keys('iphone 11')\n",
    "\n",
    "search=driver.find_element(By.CLASS_NAME,\"L0Z3Pu\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605894f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running through loop to scrap 10 pages \n",
    "rating=[]\n",
    "short_review=[]\n",
    "full_review=[]\n",
    "for i in range(1,20):\n",
    "    for a in driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]'): # ratings\n",
    "        rating.append(a.text.split(\"\\n\")[0])\n",
    "    for b in driver.find_elements(By.XPATH,\"//p[@class='_2-N8zT']\"): #short_review\n",
    "        short_review.append(b.text)\n",
    "    for c in driver.find_elements(By.XPATH,\"//div[@class='t-ZTKy']\"): # full review\n",
    "        full_review.append(c.text.replace(\"\\n\",\" \"))\n",
    "    time.sleep(3)   \n",
    "    url=\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=\"+str(i)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5b3bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the length\n",
    "print(len(rating),len(short_review),len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b8160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe\n",
    "iphone_df=pd.DataFrame({})\n",
    "iphone_df['Rating']=rating[:100]\n",
    "iphone_df[' Review summary']=short_review[:100]\n",
    "iphone_df[' Full review']=full_review[:100]\n",
    "iphone_df[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bfd6a8",
   "metadata": {},
   "source": [
    "# Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the\n",
    "search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "As shown in the image, you have to scrape the tick marked attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1834a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les first connect tot he driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\User\\Downloads\\chromedriver_win32 (1)\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.flipkart.com \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57e3a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "product=driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "product.send_keys('sneakers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad780dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.CLASS_NAME,\"_34RNph\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a94a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brands = []\n",
    "Product_Description = []\n",
    "Price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3c0347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#capturing the data and navigating through next pages\n",
    "\n",
    "#create empty lists\n",
    "brand_list=[]\n",
    "description_list=[]\n",
    "price_list=[]\n",
    "\n",
    "#there are 40 sunglasses on a single page & we need 100 records hence use a for loop ranging from 1 to 4\n",
    "for i in range(1,4):\n",
    "    for p in driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]'): # brand\n",
    "        brand_list.append(p.text)\n",
    "    for q in driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]'): # product description\n",
    "        if q.text is None:\n",
    "            description_list.append('--')  # for some products description is missing\n",
    "        else:\n",
    "            description_list.append(q.text)\n",
    "    for r in driver.find_elements(By.XPATH,\"//div[@class='_30jeq3']\"): #price\n",
    "        price_list.append(r.text)\n",
    "        time.sleep(3)    # allow some time for the driver to gather info\n",
    "    #url for the next page +str as the url changes for every page and i is from 1 to 4 it will take 3 pages\n",
    "    url = 'https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page='+str(i)\n",
    "    driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba8af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the list sizes,the description list contains various data\n",
    "print(len(brand_list),len(description_list),len(price_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123154ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe\n",
    "df=pd.DataFrame({})\n",
    "df['Brand']=brand_list[0:100]\n",
    "df['Description']=description_list[0:100]\n",
    "df['Price']=price_list[0:100]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554a01cc",
   "metadata": {},
   "source": [
    "# Q7: Go to the link - https://www.myntra.com/shoes\n",
    "Set second Price filter and Color filter to “Black”, as shown in the below image.\n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe\n",
    "description, price of the shoe as shown in the below image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dd2056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les first connect tot he driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\User\\Downloads\\chromedriver_win32 (1)\\chromedriver.exe\")\n",
    "\n",
    "driver.get('https://www.myntra.com/shoes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a48d3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying color filter\n",
    "color_filter=driver.find_element(By.XPATH,\"//label[@class='common-customCheckbox']\")\n",
    "color_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600041c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_filter=driver.find_element(By.XPATH,'/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label')\n",
    "price_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running through a loop\n",
    "\n",
    "# step 1: create empty list\n",
    "brand=[]\n",
    "description=[]\n",
    "price=[]\n",
    "\n",
    "for i in range(0,3):\n",
    "    #brand\n",
    "    for x in driver.find_elements(By.XPATH,'//h3[@class=\"product-brand\"]'):\n",
    "           brand.append(x.text)\n",
    "    \n",
    "    #description\n",
    "    for y in driver.find_elements(By.XPATH,\"//h4[@class='product-product']\"):\n",
    "        description.append(y.text)\n",
    "        \n",
    "    #price\n",
    "    for z in driver.find_elements(By.XPATH,\"//div[@class='product-price']\"):\n",
    "        price.append(z.text)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # click on next button\n",
    "    \n",
    "    next_button=driver.find_element(By.XPATH,\"//li[@class='pagination-next']\")\n",
    "    next_button.click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87faaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe\n",
    "shoe_df=pd.DataFrame({})\n",
    "shoe_df['Brand']=brand\n",
    "shoe_df['Description']=description\n",
    "shoe_df['Price in Rupees']=price\n",
    "shoe_df[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61d99bf",
   "metadata": {},
   "source": [
    "# Q8: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fc2fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les first connect tot he driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\User\\Downloads\\chromedriver_win32 (1)\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.amazon.in/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557dcaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#typing as laptop in search bar\n",
    "search_product=driver.find_element(By.ID,\"twotabsearchtextbox\")\n",
    "search_product.send_keys(\"laptop\")\n",
    "\n",
    "#click on the search button\n",
    "search_button=driver.find_element(By.ID,\"nav-search-submit-button\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01ed62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#click the checkbox for filter intel core i7\n",
    "core7=driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[5]/ul[4]/li[10]/span\")\n",
    "core7.click() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b87ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#title of the laptops\n",
    "title=driver.find_elements(By.XPATH,\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "title_list=[]\n",
    "for i in title:\n",
    "    title_list.append(i.text)\n",
    "title_list[0:2]\n",
    "\n",
    "#prices of the laptops\n",
    "price=driver.find_elements(By.XPATH,\"//span[@class='a-price-whole']\")\n",
    "price_list=[]\n",
    "for i in price:\n",
    "    price_list.append(i.text)\n",
    "price_list[0:2]\n",
    "\n",
    "#ratings \n",
    "rating=driver.find_elements(By.XPATH,\"//a[@class='a-popover-trigger a-declarative']\")\n",
    "rating_list=[]\n",
    "for i in rating:\n",
    "    if i.text is None:\n",
    "        rating_list.append(\"--\")\n",
    "    else:\n",
    "        rating_list.append(i.get_attribute(\"text\"))\n",
    "rating_list[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6683cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rating_list),len(price_list),len(title_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0269099",
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop_df=pd.DataFrame({})\n",
    "laptop_df['Title']=title_list[0:10]\n",
    "laptop_df['Rating']=rating_list[0:10]\n",
    "laptop_df['Price']=price_list[0:10]\n",
    "laptop_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8596ffb7",
   "metadata": {},
   "source": [
    "# Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida\n",
    "location. You have to scrape company name, No. of days ago when job was posted, Rating of the company.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the Job option as shown in the image.\n",
    "3. After reaching to the next webpage, In place of “Search by Designations, Companies, Skills” enter\n",
    "“Data Scientist” and click on search button.\n",
    "4. You will reach to the following web page click on location and in place of “Search location” enter\n",
    "“Noida” and select location “Noida”.\n",
    "5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231acd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first connect to the driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\User\\Downloads\\chromedriver_win32 (1)\\chromedriver.exe\")\n",
    "\n",
    "# connect to webpage:\n",
    "driver.get(\"https://www.ambitionbox.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec25b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_tab=driver.find_element(By.XPATH,\"/html/body/div/div/div/div[1]/header/nav/ul/li[5]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea7633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.XPATH,'//*[@id=\"jobs-typeahead\"]/span/input')\n",
    "designation.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24468c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit=driver.find_element(By.XPATH,'//*[@id=\"jobs\"]/div[2]/div[1]/div[1]/div/div/div/button')\n",
    "submit.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a132e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_btn=driver.find_element(By.XPATH,'//*[@id=\"filters-row\"]/div/div/div[2]/div[1]')\n",
    "location_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7980d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.XPATH,'//*[@id=\"filters-row\"]/div/div/div[2]/div[2]/div/div[2]/input')\n",
    "search.send_keys('Noida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c46540",
   "metadata": {},
   "outputs": [],
   "source": [
    "company=[]\n",
    "date =[]\n",
    "company_rating=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76df1b46",
   "metadata": {},
   "source": [
    "# Scraping Data from Ambition Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8f0a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(3)\n",
    "company_name=driver.find_elements(By.XPATH,'//p[@class=\"company body-medium\"]')\n",
    "time.sleep(5)\n",
    "\n",
    "Posted_Date=driver.find_elements(By.XPATH,'//*[@id=\"jobsList\"]/div[2]/div[2]/div/div/div/div[3]/span[1]')\n",
    "time.sleep(5)\n",
    "\n",
    "Rating=driver.find_elements(By.XPATH,'//a[@class=\"rating rating-4\"]')\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de3f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in company_name:\n",
    "    a=i.text\n",
    "    company.append(a)\n",
    "    time.sleep(2)\n",
    "for i in Posted_Date:\n",
    "    b=i.text\n",
    "    date.append(b)\n",
    "    time.sleep(2)\n",
    "for i in Rating:\n",
    "    c=i.text\n",
    "    company_rating.append(c)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070098e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(company),len(date),len(company_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee876cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_scientist_df=pd.DataFrame({})\n",
    "Data_scientist_df['COMPANY']=company[:10]\n",
    "Data_scientist_df['Posted Ago']=date[:10]\n",
    "Data_scientist_df['Rating']=company_rating[:10]\n",
    "Data_scientist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcfcdbd",
   "metadata": {},
   "source": [
    "# Q10: Write a python program to scrape the salary data for Data Scientist designation.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary.\n",
    "The above task will be, done as shown in the below steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the salaries option as shown in the image.\n",
    "3. After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and\n",
    "then click on “Data Scientist”.\n",
    "You have to scrape the data ticked in the above image.\n",
    "4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average\n",
    "salary, minimum salary, maximum salary, experience required.\n",
    "5. Store the data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47f6d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "from selenium.webdriver.support.ui import Select\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29ad2fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first connect to the driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\User\\Downloads\\chromedriver_win32 (1)\\chromedriver.exe\")\n",
    "\n",
    "# connect to webpage:\n",
    "driver.get(\"https://www.ambitionbox.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb7d25f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_click=driver.find_element(By.XPATH,'//li[@class=\"navItem\"][3]')\n",
    "salary_click.click()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50a9ad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.XPATH,'//*[@id=\"jobProfileSearchbox\"]')\n",
    "designation.send_keys('Data Scientist')\n",
    "time.sleep(2)\n",
    "submit=driver.find_element(By.XPATH,'//*[@id=\"salaries\"]/main/section[1]/div[2]/div[1]/span/div/div/div[1]/div')\n",
    "submit.click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dbe12ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name = []\n",
    "Number_of_salaries=[]\n",
    "Average_salary=[]\n",
    "Min_salary=[]\n",
    "Max_salary=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3d7cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies=driver.find_elements(By.XPATH,'//div[@class=\"company-info\"]//a')\n",
    "for i in companies:\n",
    "    company_name.append(i.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "815db73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries=driver.find_elements(By.XPATH,'//span[@class=\"datapoints\"]')\n",
    "for i in salaries:\n",
    "    Number_of_salaries.append(i.text.replace('based on ',''))\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6f87857",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_salary=driver.find_elements(By.XPATH,'//p[@class=\"averageCtc\"]')\n",
    "for i in avg_salary:\n",
    "    Average_salary.append(i.text.strip('\\nData Scientist salary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "653112ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_salaries=driver.find_elements(By.XPATH,'//div[@class=\"salary-values\"]/div[1]')\n",
    "for i in min_salaries:\n",
    "    Min_salary.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbde4643",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_salaries=driver.find_elements(By.XPATH,'//div[@class=\"salary-values\"]/div[2]')\n",
    "for i in max_salaries:\n",
    "    Max_salary.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "251dbcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(company_name),len(Number_of_salaries),len( Average_salary),len(Min_salary),len(Max_salary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6656b439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>Number_of_salaries</th>\n",
       "      <th>Average_salary</th>\n",
       "      <th>Min_salary</th>\n",
       "      <th>Max_salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart\\nData Scientist Salary</td>\n",
       "      <td>(23 salaries)</td>\n",
       "      <td>₹ 32.3L</td>\n",
       "      <td>₹ 25.0L</td>\n",
       "      <td>₹ 45.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ab Inbev\\nData Scientist Salary</td>\n",
       "      <td>(57 salaries)</td>\n",
       "      <td>₹ 19.9L</td>\n",
       "      <td>₹ 15.0L</td>\n",
       "      <td>₹ 26.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Optum\\nData Scientist Salary</td>\n",
       "      <td>(49 salaries)</td>\n",
       "      <td>₹ 16.4L</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 22.6L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZS\\nData Scientist Salary</td>\n",
       "      <td>(34 salaries)</td>\n",
       "      <td>₹ 15.8L</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 22.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fractal Analytics\\nData Scientist Salary</td>\n",
       "      <td>(115 salaries)</td>\n",
       "      <td>₹ 15.4L</td>\n",
       "      <td>₹ 9.0L</td>\n",
       "      <td>₹ 23.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tiger Analytics\\nData Scientist Salary</td>\n",
       "      <td>(68 salaries)</td>\n",
       "      <td>₹ 14.7L</td>\n",
       "      <td>₹ 9.0L</td>\n",
       "      <td>₹ 20.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sigmoid Analytics\\nData Scientist Salary</td>\n",
       "      <td>(10 salaries)</td>\n",
       "      <td>₹ 14.7L</td>\n",
       "      <td>₹ 12.7L</td>\n",
       "      <td>₹ 19.7L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Legato Health Technologies\\nData Scientist Salary</td>\n",
       "      <td>(11 salaries)</td>\n",
       "      <td>₹ 14.5L</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 20.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HSBC\\nData Scientist Salary</td>\n",
       "      <td>(10 salaries)</td>\n",
       "      <td>₹ 14.0L</td>\n",
       "      <td>₹ 12.0L</td>\n",
       "      <td>₹ 18.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tredence\\nData Scientist Salary</td>\n",
       "      <td>(14 salaries)</td>\n",
       "      <td>₹ 13.9L</td>\n",
       "      <td>₹ 8.8L</td>\n",
       "      <td>₹ 17.5L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        company_name Number_of_salaries  \\\n",
       "0                     Walmart\\nData Scientist Salary      (23 salaries)   \n",
       "1                    Ab Inbev\\nData Scientist Salary      (57 salaries)   \n",
       "2                       Optum\\nData Scientist Salary      (49 salaries)   \n",
       "3                          ZS\\nData Scientist Salary      (34 salaries)   \n",
       "4           Fractal Analytics\\nData Scientist Salary     (115 salaries)   \n",
       "5             Tiger Analytics\\nData Scientist Salary      (68 salaries)   \n",
       "6           Sigmoid Analytics\\nData Scientist Salary      (10 salaries)   \n",
       "7  Legato Health Technologies\\nData Scientist Salary      (11 salaries)   \n",
       "8                        HSBC\\nData Scientist Salary      (10 salaries)   \n",
       "9                    Tredence\\nData Scientist Salary      (14 salaries)   \n",
       "\n",
       "  Average_salary Min_salary Max_salary  \n",
       "0        ₹ 32.3L    ₹ 25.0L    ₹ 45.0L  \n",
       "1        ₹ 19.9L    ₹ 15.0L    ₹ 26.0L  \n",
       "2        ₹ 16.4L    ₹ 11.0L    ₹ 22.6L  \n",
       "3        ₹ 15.8L    ₹ 11.0L    ₹ 22.0L  \n",
       "4        ₹ 15.4L     ₹ 9.0L    ₹ 23.0L  \n",
       "5        ₹ 14.7L     ₹ 9.0L    ₹ 20.0L  \n",
       "6        ₹ 14.7L    ₹ 12.7L    ₹ 19.7L  \n",
       "7        ₹ 14.5L    ₹ 11.0L    ₹ 20.0L  \n",
       "8        ₹ 14.0L    ₹ 12.0L    ₹ 18.0L  \n",
       "9        ₹ 13.9L     ₹ 8.8L    ₹ 17.5L  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating data frame\n",
    "df=pd.DataFrame({'company_name':company_name,'Number_of_salaries':Number_of_salaries,'Average_salary':Average_salary,'Min_salary':Min_salary,'Max_salary':Max_salary})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e5a88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
